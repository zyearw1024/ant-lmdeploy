ARG LMDEPLOY_VERSION=0.6.1.1

# Stage 1: Build the WHL file
FROM nvcr.io/nvidia/pytorch:24.02-py3 AS builder

# Ubuntu 22.04 including Python 3.10
# NVIDIA CUDA 12.3.2
# NVIDIA cuBLAS 12.3.4.1
# NVIDIA cuDNN 9.0.0.306
# NVIDIA NCCL 2.19.4
# NVIDIA RAPIDS™ 23.12
# rdma-core 39.0
# NVIDIA HPC-X 2.16rc4
# OpenMPI 4.1.4+
# GDRCopy 2.3
# TensorBoard 2.9.0
# Nsight Compute 2023.3.1.1
# Nsight Systems 2023.4.1.97
# NVIDIA TensorRT™ 8.6.3
# Torch-TensorRT 2.2.0a0
# NVIDIA DALI® 1.34
# MAGMA 2.6.2
# JupyterLab 2.3.2 including Jupyter-TensorBoard
# TransformerEngine 1.3
# PyTorch quantization wheel 2.1.2
ARG LMDEPLOY_VERSION
ENV LMDEPLOY_VERSION=${LMDEPLOY_VERSION}
RUN echo "Stage 1 LMDEPLOY_VERSION: ${LMDEPLOY_VERSION}"

# Set environment variables
ENV TZ=Asia/Shanghai
ENV LOG_LEVEL=INFO
ENV CUDA_VISIBLE_DEVICES=0
ENV WRITE_WHL="true"

# Copy necessary files
COPY ./../ /ant_lmdeploy
COPY ./../builder/manywheel/entrypoint_build_ngc.sh /entrypoint_build.sh

# Build the WHL file
RUN sh /entrypoint_build.sh

# List contents of /tmpbuild for debugging
RUN ls -la /tmpbuild/

# Verify the WHL file
RUN WHL_FILE=$(ls /tmpbuild/lmdeploy-${LMDEPLOY_VERSION}*.whl) && \
    echo "Found WHL file: ${WHL_FILE}"

# List contents of /lmdeploy_build for debugging
RUN ls -la /lmdeploy_build/

# Stage 2: Create a minimal stage to copy the WHL file
FROM scratch AS exporter
COPY --from=builder /lmdeploy_build/*.whl .

# Stage 3: Create the final image
FROM nvcr.io/nvidia/pytorch:24.02-py3

ARG LMDEPLOY_VERSION
ENV LMDEPLOY_VERSION=${LMDEPLOY_VERSION}

# Set CUDA architecture list
ARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'
ENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}

# Install dependencies
COPY ./../requirements/ngc-build.txt /workspace/requirements-ngc-build.txt
RUN pip3 install -r /workspace/requirements-ngc-build.txt

# Display detailed information about the LMDEPLOY_VERSION for verification
RUN echo "Stage 3 LMDEPLOY_VERSION: ${LMDEPLOY_VERSION}" && \
    echo "Current CUDA architecture list: ${TORCH_CUDA_ARCH_LIST}" && \
    echo "Environment variables set: TZ=${TZ}, LOG_LEVEL=${LOG_LEVEL}, CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}, WRITE_WHL=${WRITE_WHL}"

# Copy and install the WHL file
COPY --from=builder /tmpbuild/lmdeploy-${LMDEPLOY_VERSION}*.whl /workspace/

# Ensure the WHL file is correctly specified
RUN WHL_FILE=$(ls /workspace/lmdeploy-${LMDEPLOY_VERSION}*.whl) && \
    echo "Installing WHL file: ${WHL_FILE}" && \
    if [ -n "$WHL_FILE" ]; then pip3 install "$WHL_FILE" --no-deps; else echo "No WHL file found"; exit 1; fi

 # # Install triton
# RUN pip3 install triton==2.1.0

WORKDIR /workspace


# Example build commands

# Step 1: If not in the docker directory, navigate to it first
# cd docker

# Step 2: Build and export the WHL file locally
# This step builds the WHL file and exports it to the local ./lmdeploy_build directory
# time DOCKER_BUILDKIT=1 docker build --progress=plain --platform linux/amd64 --build-arg LMDEPLOY_VERSION=0.6.1.3   --target exporter --output type=local,dest=./lmdeploy_build -f Dockerfile_ngc ..

# Step 3: Build the final image
# This step builds the complete image, including installing the WHL file
# time DOCKER_BUILDKIT=1 docker build --progress=plain --platform linux/amd64 -t ant_lmdeploy:v0.6.1.3_cu123_$(date +"%Y%m%d") --build-arg LMDEPLOY_VERSION=0.6.1.3 -f Dockerfile_ngc ..
